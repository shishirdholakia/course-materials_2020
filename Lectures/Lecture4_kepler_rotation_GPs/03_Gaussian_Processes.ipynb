{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common need in time series analysis is to make **predictions about what we expect to observe**.  This can be interpolations or extrapolations (ie. forecasts).  One utility of doing this is that we can use the prediction to help guide future observations. We can also use predictions to resample noisy irregular time-series data onto an equally spaced time grid, and draw as many instantiations of our object from within the uncertainty distribution.\n",
    "\n",
    "Classical statistics has developed a family of approaches (parametric probabilistic forecasting) such as Autoregressive integrated moving average ([ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)), Exponential smoothing methods (e.g., Holt-Winters) and [Theta](https://robjhyndman.com/papers/Theta.pdf). Clearly if we have a perfect (physical) model of what produces what we observe, then that will always be better than using a fully data-driven model. However, even if we know a lot about the underlying model that gives rise to the observations we see, there are some complications:\n",
    "\n",
    "- We likely dont know **all** the relevant model parameters.\n",
    "- Building models may be extremely computationally expensive (e.g., finite element analysis).\n",
    "- Forward-folding a model into observations is non-trivial \n",
    "\n",
    "Most machine learning algorithms do not naturally take uncertainty of observations into account. Nonparametric probabilistic forecasting **Gaussian processess** is one family of data-driven modelling approaches that can take uncertainties into account and can, in turn, provide realistic prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "<img src=\"imgs/gp.png\">\n",
    "From: C. E. Rasmussen & C. K. I. Williams, [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "Let's make an extremely simple time series curve. Assume I have a prior belief that two data points ($x_1$ and $x_2$) separated in time will have some mean and some (Gaussian) variance (figures from Roberts et al. 2013, https://doi.org/10.1098/rsta.2011.0550).\n",
    "\n",
    "If I know what the covariance is between $x_1$ and $x_2$ then observing $x_1$ allows me to narrow my guess of what $x_2$ will be.\n",
    "\n",
    "<img src=\"imgs/rsta20110550f02.jpg\" width=\"50%\">\n",
    "\n",
    "<img src=\"imgs/rsta20110550f03.jpg\" width=\"80%\">\n",
    "\n",
    "\n",
    "\n",
    "The idea of gaussian processes is to extend this simple example to multiple dimensions, and set a functional form for the covariance between $x_i$ and $x_j$. Let's observe at time 2, 6, and 8 (a), then extend this to smaller observational windows (b). \n",
    "\n",
    "<img src=\"imgs/rsta20110550f04.jpg\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A Gaussian process is a generalization of the\n",
    "Gaussian probability distribution. Whereas a probability distribution describes\n",
    "random variables which are scalars or vectors (for multivariate distributions),\n",
    "a stochastic process governs the properties of functions. Leaving mathematical\n",
    "sophistication aside, one can loosely think of a function as a very long vector,\n",
    "each entry in the vector specifying the function value f(x) at a particular input\n",
    "x\"\n",
    "\n",
    "<img src=\"http://www.gaussianprocess.org/gpml/rwcover.gif\">\n",
    "http://www.gaussianprocess.org/\n",
    "\n",
    "(see also http://katbailey.github.io/post/gaussian-processes-for-dummies/, https://github.com/adamian/adamian.github.io/blob/master/talks/Damianou_16_MIT.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following http://keyonvafa.com/gp-tutorial/: \n",
    "\n",
    "\"GP is specified by a mean function $m(x)$ and a covariance function $k(x,x‚Ä≤)$, otherwise known as a kernel. That is, for any $x,x‚Ä≤$, $m(x)=ùîº[f(x)]$ and $k(x,x‚Ä≤)=Cov(f(x),f(x‚Ä≤))$.\"\n",
    " \n",
    "say\n",
    "$$\n",
    "k(x,x‚Ä≤)=h^2 \\left(1+\\frac{(x‚àíx‚Ä≤)^2}{2Œ±l^2}\\right)^{‚àíŒ±},\n",
    "$$\n",
    "we can plot some functions (for $h=1$ and $m(x) = 0$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "def rq_covariance(params, x, xp):\n",
    "    h= params[0]\n",
    "    Œ± = params[1]\n",
    "    l = params[2]\n",
    "    diffs = np.expand_dims(x /l, 1) - np.expand_dims(xp/l, 0)\n",
    "    return h**2 *np.power(1+np.sum(diffs**2, axis=2)/(2*Œ±*l**2),(-1*Œ±)) \n",
    "\n",
    "def plot_gp(ax,params, plot_xs, n_samples = 10, xlab=False, ylab=False):\n",
    "    plot_xs = np.reshape(np.linspace(-5, 5, 300), (300,1))\n",
    "    sampled_funcs = np.random.multivariate_normal(np.ones(len(plot_xs)), \n",
    "                                                  rq_covariance(params,plot_xs,plot_xs), size=10)\n",
    "    ax.plot(plot_xs, sampled_funcs.T)\n",
    "    ax.set_title(r'$\\alpha = {},\\/ l = {} $'.format(\\\n",
    "        params[1],params[2]),fontsize = 22)\n",
    "    if xlab:\n",
    "        ax.set_xlabel(r'$x$',fontsize = 20)\n",
    "    if ylab:\n",
    "        ax.set_ylabel(r'$f(x)$',fontsize = 20)\n",
    "\n",
    "fig = plt.figure(figsize=(20,8), facecolor='white')\n",
    "ax_1 = fig.add_subplot(231, frameon=False)\n",
    "ax_2 = fig.add_subplot(232, frameon=False)\n",
    "ax_3 = fig.add_subplot(233, frameon=False)\n",
    "ax_4 = fig.add_subplot(234, frameon=False)\n",
    "ax_5 = fig.add_subplot(235, frameon=False)\n",
    "ax_6 = fig.add_subplot(236, frameon=False)\n",
    "ax_1.set_xticks([])\n",
    "ax_1.set_yticks([])\n",
    "ax_2.set_xticks([])\n",
    "ax_2.set_yticks([])\n",
    "ax_3.set_xticks([])\n",
    "ax_3.set_yticks([])\n",
    "ax_4.set_xticks([])\n",
    "ax_4.set_yticks([])\n",
    "ax_5.set_xticks([])\n",
    "ax_5.set_yticks([])\n",
    "ax_6.set_xticks([])\n",
    "ax_6.set_yticks([])\n",
    "\n",
    "plot_xs = np.reshape(np.linspace(-5, 5, 300), (300,1))\n",
    "plot_gp(ax_1,np.array([1,1,.5]), plot_xs,ylab=True)\n",
    "plot_gp(ax_2,np.array([1,1,1.0]), plot_xs)\n",
    "plot_gp(ax_3,np.array([1,1,2.0]), plot_xs)\n",
    "plot_gp(ax_4,np.array([1,.02,1]), plot_xs,ylab=True,xlab=True)\n",
    "plot_gp(ax_5,np.array([1,.1,1]), plot_xs,xlab=True)\n",
    "plot_gp(ax_6,np.array([1,2.0,1]), plot_xs,xlab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the kernels look like?\n",
    "\n",
    "<img src=\"imgs/rational_quad.png\">\n",
    "\n",
    "Source: https://peterroelants.github.io/posts/gaussian-process-kernel-fitting/\n",
    "\n",
    "By measuring value of $f(x)$ for some finite $x$ we can pin down viable values of possible functions that are consistent with the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RationalQuadratic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  First the noiseless case\n",
    "X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n",
    "\n",
    "# Observations\n",
    "y = f(X).ravel()\n",
    "\n",
    "# Mesh the input space for evaluations of the real function, the prediction and\n",
    "# its MSE\n",
    "x = np.atleast_2d(np.linspace(0, 10, 1000)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gaussian Process model\n",
    "kernel = C(2.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=1.5, alpha=1.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(X, y)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "y_pred, sigma = gp.predict(x, return_std=True)\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval based on\n",
    "# the MSE\n",
    "fig = plt.figure()\n",
    "plt.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# now the noisy case\n",
    "X = np.linspace(0.1, 9.9, 20)\n",
    "X = np.atleast_2d(X).T\n",
    "\n",
    "# Observations and noise\n",
    "y = f(X).ravel()\n",
    "dy = 0.5 + 1.0 * np.random.random(y.shape)\n",
    "noise = np.random.normal(0, dy)\n",
    "y += noise\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RationalQuadratic(length_scale=0.5,alpha=0.5)\n",
    "\n",
    "\n",
    "# Instanciate a Gaussian Process model\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=(dy / y) ** 2,\n",
    "                              n_restarts_optimizer=10)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(X, y)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "y_pred, sigma = gp.predict(x, return_std=True)\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval based on\n",
    "# the MSE\n",
    "fig = plt.figure()\n",
    "plt.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y_pred, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world example: Light curve classification challenge from LSST (hosted on Kaggle).  UC Berkeley physics postdoc Kyle Boone won the competition using Gaussian Processes.  The following are slides from his talk last month (https://www.ml4science.org/ml-seminar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/sn_raw.png\">\n",
    "\n",
    "<img src=\"imgs/sn_gp1.png\">\n",
    "\n",
    "<img src=\"imgs/sn_gp2.png\">\n",
    "\n",
    "<img src=\"imgs/sn_gp3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, our data can be periodic and then it makes sense to use a periodic kernel:\n",
    "\n",
    "<img src=\"imgs/periodic_kernel.png\">\n",
    "Source: https://peterroelants.github.io/posts/gaussian-process-kernel-fitting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
